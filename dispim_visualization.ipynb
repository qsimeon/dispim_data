{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diSPIM Data Visualization Pipeline\n",
    "\n",
    "This notebook provides a complete pipeline to load, process, and visualize high-resolution microscopy data from a double diSPIM (dual-view selective plane illumination microscopy) system.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The data consists of:\n",
    "- **Two imaging arms**: Alpha and Beta (each diSPIM)\n",
    "- **Two channels per arm**: Two cameras/sides per diSPIM\n",
    "- **Z-stacks**: 200 slices per volume\n",
    "- **High resolution**: 2304×2304 pixels per slice\n",
    "- **Format**: OME-TIFF files with detailed JSON metadata\n",
    "\n",
    "## Goals\n",
    "\n",
    "1. Load OME-TIFF image stacks from both alpha and beta arms\n",
    "2. Extract and parse metadata for temporal and spatial alignment\n",
    "3. Create side-by-side video visualizations\n",
    "4. Enable interactive exploration of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports\n",
    "\n",
    "First, we import all necessary libraries for data loading, processing, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import tifffile\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import imageio\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib for inline display\n",
    "# Use widget backend for interactive plots\n",
    "%matplotlib notebook  \n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Metadata Parser\n",
    "\n",
    "The metadata files contain crucial information about the acquisition parameters. We need to parse the JSON structure to extract:\n",
    "- Image dimensions (width, height, slices, channels)\n",
    "- Temporal information (start time, slice period, volume duration)\n",
    "- Spatial information (pixel size, z-step, position)\n",
    "- Acquisition settings (delay between sides, etc.)\n",
    "\n",
    "The metadata file has a nested structure with a \"Summary\" key containing most important parameters, and a \"SPIMAcqSettings\" field that contains a JSON string with additional settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata parser test successful!\n",
      "Image dimensions: 2304x2304x200\n",
      "Channels: 2 (HamCam2, HamCam1)\n",
      "Slice period: 12.5 ms\n",
      "Volume duration: 5.0255 s\n"
     ]
    }
   ],
   "source": [
    "def parse_metadata(metadata_path):\n",
    "    \"\"\"\n",
    "    Parse the metadata JSON file and extract key acquisition parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    metadata_path : str or Path\n",
    "        Path to the metadata.txt file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing parsed metadata parameters\n",
    "    \"\"\"\n",
    "    # Try different encodings to handle files that may not be UTF-8\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "    metadata = None\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(metadata_path, 'r', encoding=encoding, errors='replace') as f:\n",
    "                metadata = json.load(f)\n",
    "            break\n",
    "        except (UnicodeDecodeError, json.JSONDecodeError):\n",
    "            continue\n",
    "    \n",
    "    if metadata is None:\n",
    "        # Last resort: try with errors='ignore' and latin-1 (can decode any byte)\n",
    "        with open(metadata_path, 'r', encoding='latin-1', errors='ignore') as f:\n",
    "            metadata = json.load(f)\n",
    "    \n",
    "    summary = metadata.get('Summary', {})\n",
    "    \n",
    "    # Parse the nested SPIMAcqSettings JSON string\n",
    "    spim_settings_str = summary.get('SPIMAcqSettings', '{}')\n",
    "    try:\n",
    "        spim_settings = json.loads(spim_settings_str)\n",
    "    except:\n",
    "        spim_settings = {}\n",
    "    \n",
    "    # Extract key parameters\n",
    "    parsed = {\n",
    "        # Image dimensions\n",
    "        'width': int(summary.get('Width', 0)),\n",
    "        'height': int(summary.get('Height', 0)),\n",
    "        'slices': int(summary.get('Slices', 0)),\n",
    "        'channels': int(summary.get('Channels', 0)),\n",
    "        'frames': int(summary.get('Frames', 1)),\n",
    "        \n",
    "        # Channel information\n",
    "        'channel_names': summary.get('ChNames', []),\n",
    "        'slices_first': summary.get('SlicesFirst', 'true').lower() == 'true',\n",
    "        'time_first': summary.get('TimeFirst', 'false').lower() == 'true',\n",
    "        \n",
    "        # Spatial information\n",
    "        'pixel_size_um': float(summary.get('PixelSize_um', 0)),\n",
    "        'z_step_um': float(summary.get('z-step_um', 0)),\n",
    "        'position_x': summary.get('Position_X', '0'),\n",
    "        'position_y': summary.get('Position_Y', '0'),\n",
    "        \n",
    "        # Temporal information\n",
    "        'start_time': summary.get('StartTime', ''),\n",
    "        'slice_period_ms': float(summary.get('SlicePeriod_ms', '0 ms').split()[0]),\n",
    "        'volume_duration_sec': float(summary.get('VolumeDuration', '0 s').split()[0]),\n",
    "        \n",
    "        # Acquisition settings\n",
    "        'delay_before_side': spim_settings.get('delayBeforeSide', 0.25),\n",
    "        'num_sides': spim_settings.get('numSides', 2),\n",
    "        'first_side_is_a': spim_settings.get('firstSideIsA', True),\n",
    "        \n",
    "        # Other useful info\n",
    "        'acquisition_name': summary.get('AcquisitionName', ''),\n",
    "        'date': summary.get('Date', ''),\n",
    "        'pixel_type': summary.get('PixelType', ''),\n",
    "        'bit_depth': int(summary.get('BitDepth', 16)),\n",
    "        'mv_rotations': summary.get('MVRotations', ''),\n",
    "        \n",
    "        # Full metadata for reference\n",
    "        'raw_summary': summary,\n",
    "        'raw_spim_settings': spim_settings\n",
    "    }\n",
    "    \n",
    "    return parsed\n",
    "\n",
    "# Test the parser\n",
    "test_metadata_path = Path('1msec_worm/I/beads_alpha_worm2/beads_alpha_worm2_MMStack_Pos0_metadata.txt')\n",
    "if test_metadata_path.exists():\n",
    "    test_meta = parse_metadata(test_metadata_path)\n",
    "    print(\"Metadata parser test successful!\")\n",
    "    print(f\"Image dimensions: {test_meta['width']}x{test_meta['height']}x{test_meta['slices']}\")\n",
    "    print(f\"Channels: {test_meta['channels']} ({', '.join(test_meta['channel_names'])})\")\n",
    "    print(f\"Slice period: {test_meta['slice_period_ms']} ms\")\n",
    "    print(f\"Volume duration: {test_meta['volume_duration_sec']} s\")\n",
    "else:\n",
    "    print(\"Test metadata file not found. Parser function is ready to use.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: OME-TIFF Loader\n",
    "\n",
    "The OME-TIFF files contain multi-dimensional image stacks. Based on the metadata:\n",
    "- Data organization: `SlicesFirst=True` means data is organized as [Slices, Channels, Height, Width]\n",
    "- Each arm has 2 channels (two cameras/sides)\n",
    "- 200 slices per volume\n",
    "- 2304×2304 pixels per slice\n",
    "- 16-bit grayscale data\n",
    "\n",
    "We need to load the data and properly reshape it to separate channels and slices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing OME-TIFF loader...\n",
      "Loading OME-TIFF: 1msec_worm/I/beads_alpha_worm2/beads_alpha_worm2_MMStack_Pos0.ome.tif\n",
      "Raw data shape: (200, 2, 2304, 2304)\n",
      "Data dtype: uint16\n",
      "Limited to 5 slices\n",
      "Final data shape: (5, 2304, 2304)\n",
      "Test load successful! Data shape: (5, 2304, 2304)\n",
      "Data range: [35, 195]\n"
     ]
    }
   ],
   "source": [
    "def load_ome_tiff(tiff_path, metadata=None, channel_idx=None, max_slices=None):\n",
    "    \"\"\"\n",
    "    Load an OME-TIFF file and return properly shaped array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tiff_path : str or Path\n",
    "        Path to the .ome.tif file\n",
    "    metadata : dict, optional\n",
    "        Parsed metadata dictionary. If provided, uses it to determine data organization.\n",
    "        If None, will try to infer from file.\n",
    "    channel_idx : int or None\n",
    "        If specified, return only this channel (0-indexed). If None, return all channels.\n",
    "    max_slices : int or None\n",
    "        If specified, load only the first max_slices slices (for memory efficiency)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray : Image data\n",
    "        Shape depends on parameters:\n",
    "        - If channel_idx specified: (slices, height, width)\n",
    "        - If channel_idx is None: (slices, channels, height, width) or (channels, slices, height, width)\n",
    "    \"\"\"\n",
    "    print(f\"Loading OME-TIFF: {tiff_path}\")\n",
    "    \n",
    "    # Load the TIFF file\n",
    "    # tifffile can handle OME-TIFF format and preserve metadata\n",
    "    with tifffile.TiffFile(tiff_path) as tif:\n",
    "        # Get the image series (OME-TIFF can have multiple series)\n",
    "        if len(tif.series) > 0:\n",
    "            # Get the first (and usually only) series\n",
    "            series = tif.series[0]\n",
    "            data = series.asarray()\n",
    "        else:\n",
    "            # Fallback: read directly\n",
    "            data = tifffile.imread(tiff_path)\n",
    "    \n",
    "    print(f\"Raw data shape: {data.shape}\")\n",
    "    print(f\"Data dtype: {data.dtype}\")\n",
    "    \n",
    "    # Determine data organization from metadata or infer from shape\n",
    "    if metadata is not None:\n",
    "        slices_first = metadata.get('slices_first', True)\n",
    "        num_slices = metadata.get('slices', 200)\n",
    "        num_channels = metadata.get('channels', 2)\n",
    "    else:\n",
    "        # Try to infer: if we have 4D data, assume [slices, channels, height, width]\n",
    "        # or [channels, slices, height, width]\n",
    "        if len(data.shape) == 4:\n",
    "            # Check which dimension is larger (slices vs channels)\n",
    "            if data.shape[0] > data.shape[1]:\n",
    "                slices_first = True\n",
    "                num_slices, num_channels = data.shape[0], data.shape[1]\n",
    "            else:\n",
    "                slices_first = False\n",
    "                num_channels, num_slices = data.shape[0], data.shape[1]\n",
    "        else:\n",
    "            # 3D or 2D - assume it's already in the right format\n",
    "            slices_first = True\n",
    "            num_slices = data.shape[0] if len(data.shape) >= 3 else 1\n",
    "            num_channels = 1\n",
    "    \n",
    "    # Limit slices if requested (for memory efficiency)\n",
    "    if max_slices is not None and num_slices > max_slices:\n",
    "        if slices_first:\n",
    "            data = data[:max_slices]\n",
    "        else:\n",
    "            data = data[:, :max_slices]\n",
    "        num_slices = max_slices\n",
    "        print(f\"Limited to {max_slices} slices\")\n",
    "    \n",
    "    # Reshape if needed\n",
    "    if len(data.shape) == 4:\n",
    "        if slices_first:\n",
    "            # Data is [slices, channels, height, width] - this is what we want\n",
    "            pass\n",
    "        else:\n",
    "            # Data is [channels, slices, height, width] - transpose\n",
    "            data = np.transpose(data, (1, 0, 2, 3))\n",
    "    elif len(data.shape) == 3:\n",
    "        # 3D data - need to determine if it's [slices, height, width] or [channels, height, width]\n",
    "        if slices_first:\n",
    "            # Assume it's [slices, height, width] with 1 channel\n",
    "            data = data[:, np.newaxis, :, :]\n",
    "        else:\n",
    "            # Assume it's [channels, height, width] with 1 slice\n",
    "            data = data[np.newaxis, :, :, :]\n",
    "            data = np.transpose(data, (1, 0, 2, 3))\n",
    "    elif len(data.shape) == 2:\n",
    "        # Single 2D image\n",
    "        data = data[np.newaxis, np.newaxis, :, :]\n",
    "    \n",
    "    # Extract specific channel if requested\n",
    "    if channel_idx is not None:\n",
    "        if len(data.shape) == 4:\n",
    "            data = data[:, channel_idx, :, :]\n",
    "        else:\n",
    "            print(f\"Warning: Cannot extract channel {channel_idx} from data shape {data.shape}\")\n",
    "    \n",
    "    print(f\"Final data shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# Test the loader (with limited slices to avoid memory issues)\n",
    "test_tiff_path = Path('1msec_worm/I/beads_alpha_worm2/beads_alpha_worm2_MMStack_Pos0.ome.tif')\n",
    "if test_tiff_path.exists():\n",
    "    print(\"Testing OME-TIFF loader...\")\n",
    "    test_meta = parse_metadata(test_tiff_path.parent / test_tiff_path.name.replace('.ome.tif', '_metadata.txt'))\n",
    "    # Load only first 5 slices and first channel for testing\n",
    "    test_data = load_ome_tiff(test_tiff_path, metadata=test_meta, channel_idx=0, max_slices=5)\n",
    "    print(f\"Test load successful! Data shape: {test_data.shape}\")\n",
    "    print(f\"Data range: [{test_data.min()}, {test_data.max()}]\")\n",
    "else:\n",
    "    print(\"Test TIFF file not found. Loader function is ready to use.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Notes on Image Display and Data Preservation\n",
    "\n",
    "**Image Quality and Dynamic Range:**\n",
    "- The display functions now preserve the **full dynamic range** of your data\n",
    "- Images are scaled linearly from their actual min/max values to [0,1] for display\n",
    "- **No clipping or percentile-based normalization** is applied by default\n",
    "- The actual pixel values are preserved in memory - only the display is scaled\n",
    "\n",
    "**Data Formats:**\n",
    "- Currently using NumPy arrays loaded from OME-TIFF files\n",
    "- For very large datasets, consider using **Zarr** or **HDF5** formats for:\n",
    "  - Memory-efficient chunked access\n",
    "  - Lazy loading of slices\n",
    "  - Better performance with large stacks\n",
    "  - Example: `zarr.open('data.zarr', mode='r')` for chunked array access\n",
    "\n",
    "**Memory Considerations:**\n",
    "- Full stacks are now loaded by default (MAX_SLICES_FOR_DISPLAY = None)\n",
    "- For 200 slices × 2304×2304 × 2 channels × 2 bytes = ~4.2 GB per arm\n",
    "- If memory is limited, set MAX_SLICES_FOR_DISPLAY to a smaller number\n",
    "- Consider using memory mapping or chunked loading for very large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 acquisition pairs:\n",
      "  1. 10msec_worm/I: alpha=beads_alpha_worm2_4, beta=beads_beta_worm2_4\n",
      "  2. 1msec_worm/I: alpha=beads_alpha_worm2, beta=beads_beta_worm2\n",
      "  3. 1msec_worm/II: alpha=beads_alpha_worm2_1_1, beta=beads_beta_worm2_1_1\n",
      "  4. 200msec_worm/I: alpha=beads_alpha_worm2, beta=beads_beta_worm2\n",
      "  5. 200msec_worm/II: alpha=beads_alpha_worm2_1, beta=beads_beta_worm2_1\n",
      "  6. 200msec_worm/III: alpha=beads_alpha_worm2_2, beta=beads_beta_worm2_2\n",
      "  7. 200msec_worm/IV: alpha=beads_alpha_worm2_3, beta=beads_beta_worm2_3\n",
      "  8. 20msec_worm/I: alpha=beads_alpha_worm2_2_1, beta=beads_beta_worm2_2_1\n"
     ]
    }
   ],
   "source": [
    "def discover_acquisitions(root_dir='.'):\n",
    "    \"\"\"\n",
    "    Discover all alpha/beta acquisition pairs in the directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    root_dir : str or Path\n",
    "        Root directory to search for acquisitions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of dictionaries, each containing:\n",
    "        - 'condition': Top-level folder name (e.g., '1msec_worm')\n",
    "        - 'run': Second-level folder name (e.g., 'I')\n",
    "        - 'alpha_path': Path to alpha folder\n",
    "        - 'beta_path': Path to beta folder\n",
    "        - 'alpha_metadata': Path to alpha metadata file\n",
    "        - 'beta_metadata': Path to beta metadata file\n",
    "        - 'alpha_tiff': Path to alpha OME-TIFF file\n",
    "        - 'beta_tiff': Path to beta OME-TIFF file\n",
    "    \"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    acquisitions = []\n",
    "    \n",
    "    # Find all top-level directories (acquisition conditions)\n",
    "    for condition_dir in sorted(root_path.iterdir()):\n",
    "        if not condition_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        condition_name = condition_dir.name\n",
    "        \n",
    "        # Find all second-level directories (runs)\n",
    "        for run_dir in sorted(condition_dir.iterdir()):\n",
    "            if not run_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            run_name = run_dir.name\n",
    "            \n",
    "            # Look for alpha and beta folders\n",
    "            alpha_folder = None\n",
    "            beta_folder = None\n",
    "            \n",
    "            for subfolder in run_dir.iterdir():\n",
    "                if not subfolder.is_dir():\n",
    "                    continue\n",
    "                \n",
    "                folder_name = subfolder.name.lower()\n",
    "                if 'alpha' in folder_name and 'beads' in folder_name:\n",
    "                    alpha_folder = subfolder\n",
    "                elif 'beta' in folder_name and 'beads' in folder_name:\n",
    "                    beta_folder = subfolder\n",
    "            \n",
    "            # If we found both alpha and beta, create an acquisition entry\n",
    "            if alpha_folder and beta_folder:\n",
    "                # Find metadata and TIFF files\n",
    "                alpha_metadata = None\n",
    "                alpha_tiff = None\n",
    "                beta_metadata = None\n",
    "                beta_tiff = None\n",
    "                \n",
    "                # Look for metadata and TIFF files in alpha folder\n",
    "                for file in alpha_folder.iterdir():\n",
    "                    if file.suffix == '.txt' and 'metadata' in file.name:\n",
    "                        alpha_metadata = file\n",
    "                    elif file.suffix == '.tif' or file.suffix == '.tiff':\n",
    "                        alpha_tiff = file\n",
    "                \n",
    "                # Look for metadata and TIFF files in beta folder\n",
    "                for file in beta_folder.iterdir():\n",
    "                    if file.suffix == '.txt' and 'metadata' in file.name:\n",
    "                        beta_metadata = file\n",
    "                    elif file.suffix == '.tif' or file.suffix == '.tiff':\n",
    "                        beta_tiff = file\n",
    "                \n",
    "                # Only add if we have all required files\n",
    "                if alpha_metadata and alpha_tiff and beta_metadata and beta_tiff:\n",
    "                    acquisitions.append({\n",
    "                        'condition': condition_name,\n",
    "                        'run': run_name,\n",
    "                        'alpha_path': alpha_folder,\n",
    "                        'beta_path': beta_folder,\n",
    "                        'alpha_metadata': alpha_metadata,\n",
    "                        'beta_metadata': beta_metadata,\n",
    "                        'alpha_tiff': alpha_tiff,\n",
    "                        'beta_tiff': beta_tiff\n",
    "                    })\n",
    "    \n",
    "    return acquisitions\n",
    "\n",
    "# Discover all acquisitions\n",
    "acquisitions = discover_acquisitions('.')\n",
    "print(f\"Found {len(acquisitions)} acquisition pairs:\")\n",
    "for i, acq in enumerate(acquisitions):\n",
    "    print(f\"  {i+1}. {acq['condition']}/{acq['run']}: alpha={acq['alpha_path'].name}, beta={acq['beta_path'].name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Temporal Alignment\n",
    "\n",
    "The alpha and beta arms may have slight timing differences. We need to:\n",
    "1. Extract start times from metadata\n",
    "2. Account for the delay between sides (`delayBeforeSide`)\n",
    "3. Calculate frame-by-frame timing\n",
    "4. Align the sequences temporally\n",
    "\n",
    "This is important for creating synchronized side-by-side videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal alignment calculated:\n",
      "  Alpha start: 2025-11-12 16:49:53-05:00\n",
      "  Beta start: 2025-11-12 16:49:51-05:00\n",
      "  Time offset: -2.000 seconds\n",
      "  Delay before side: 0.250 seconds\n",
      "  Average slice period: 21.50 ms\n",
      "  Number of slices: 200\n"
     ]
    }
   ],
   "source": [
    "def parse_start_time(time_str):\n",
    "    \"\"\"\n",
    "    Parse the StartTime string from metadata into a datetime object.\n",
    "    \n",
    "    Format: \"2025-11-12 17:04:10 -0500\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try parsing with timezone\n",
    "        dt = datetime.strptime(time_str, \"%Y-%m-%d %H:%M:%S %z\")\n",
    "        return dt\n",
    "    except:\n",
    "        try:\n",
    "            # Try without timezone\n",
    "            dt = datetime.strptime(time_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "            return dt\n",
    "        except:\n",
    "            print(f\"Warning: Could not parse time string: {time_str}\")\n",
    "            return None\n",
    "\n",
    "def calculate_temporal_alignment(alpha_meta, beta_meta):\n",
    "    \"\"\"\n",
    "    Calculate temporal alignment between alpha and beta acquisitions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha_meta : dict\n",
    "        Parsed metadata for alpha arm\n",
    "    beta_meta : dict\n",
    "        Parsed metadata for beta arm\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing:\n",
    "        - 'time_offset_sec': Time difference between start times (beta - alpha)\n",
    "        - 'alpha_start': Alpha start datetime\n",
    "        - 'beta_start': Beta start datetime\n",
    "        - 'slice_period_ms': Average slice period\n",
    "        - 'frame_times_alpha': Array of frame times for alpha (relative to alpha start)\n",
    "        - 'frame_times_beta': Array of frame times for beta (relative to beta start)\n",
    "    \"\"\"\n",
    "    alpha_start = parse_start_time(alpha_meta['start_time'])\n",
    "    beta_start = parse_start_time(beta_meta['start_time'])\n",
    "    \n",
    "    if alpha_start is None or beta_start is None:\n",
    "        print(\"Warning: Could not parse start times. Using default alignment.\")\n",
    "        time_offset_sec = 0.0\n",
    "    else:\n",
    "        # Calculate time difference (beta - alpha) in seconds\n",
    "        time_diff = beta_start - alpha_start\n",
    "        time_offset_sec = time_diff.total_seconds()\n",
    "    \n",
    "    # Get slice periods\n",
    "    alpha_slice_period = alpha_meta['slice_period_ms'] / 1000.0  # Convert to seconds\n",
    "    beta_slice_period = beta_meta['slice_period_ms'] / 1000.0\n",
    "    \n",
    "    # Use average slice period\n",
    "    avg_slice_period = (alpha_slice_period + beta_slice_period) / 2.0\n",
    "    \n",
    "    # Account for delay before side\n",
    "    delay_before_side = alpha_meta.get('delay_before_side', 0.25)\n",
    "    \n",
    "    # Calculate frame times\n",
    "    num_slices = min(alpha_meta['slices'], beta_meta['slices'])\n",
    "    \n",
    "    # Alpha frame times: starts immediately\n",
    "    frame_times_alpha = np.arange(num_slices) * alpha_slice_period\n",
    "    \n",
    "    # Beta frame times: starts after delay_before_side\n",
    "    frame_times_beta = np.arange(num_slices) * beta_slice_period + delay_before_side\n",
    "    \n",
    "    return {\n",
    "        'time_offset_sec': time_offset_sec,\n",
    "        'alpha_start': alpha_start,\n",
    "        'beta_start': beta_start,\n",
    "        'slice_period_ms': avg_slice_period * 1000,\n",
    "        'delay_before_side': delay_before_side,\n",
    "        'frame_times_alpha': frame_times_alpha,\n",
    "        'frame_times_beta': frame_times_beta,\n",
    "        'num_slices': num_slices\n",
    "    }\n",
    "\n",
    "# Test temporal alignment with first acquisition\n",
    "if len(acquisitions) > 0:\n",
    "    acq = acquisitions[0]\n",
    "    alpha_meta = parse_metadata(acq['alpha_metadata'])\n",
    "    beta_meta = parse_metadata(acq['beta_metadata'])\n",
    "    \n",
    "    alignment = calculate_temporal_alignment(alpha_meta, beta_meta)\n",
    "    print(\"Temporal alignment calculated:\")\n",
    "    print(f\"  Alpha start: {alignment['alpha_start']}\")\n",
    "    print(f\"  Beta start: {alignment['beta_start']}\")\n",
    "    print(f\"  Time offset: {alignment['time_offset_sec']:.3f} seconds\")\n",
    "    print(f\"  Delay before side: {alignment['delay_before_side']:.3f} seconds\")\n",
    "    print(f\"  Average slice period: {alignment['slice_period_ms']:.2f} ms\")\n",
    "    print(f\"  Number of slices: {alignment['num_slices']}\")\n",
    "else:\n",
    "    print(\"No acquisitions found for temporal alignment test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Spatial Information Extraction\n",
    "\n",
    "Extract spatial calibration and position information from metadata. This information will be useful for future alignment and registration steps.\n",
    "\n",
    "Key spatial parameters:\n",
    "- Pixel size (μm per pixel)\n",
    "- Z-step size (μm between slices)\n",
    "- Stage positions (X, Y)\n",
    "- Rotation information (MVRotations: \"0_90\" suggests 90° rotation between arms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial information:\n",
      "  Pixel size: 0.1220 μm/pixel\n",
      "  Z-step: 0.7000 μm/slice\n",
      "  Physical volume: 281.09 × 281.09 × 140.00 μm\n",
      "  Alpha position: (-0.000, -0.000) μm\n",
      "  Beta position: (-0.000, 0.100) μm\n",
      "  MVRotations - Alpha: 0_90, Beta: 0_90\n",
      "  Note: 90° rotation detected between arms (typical for diSPIM)\n"
     ]
    }
   ],
   "source": [
    "def extract_spatial_info(alpha_meta, beta_meta):\n",
    "    \"\"\"\n",
    "    Extract spatial calibration and position information.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha_meta : dict\n",
    "        Parsed metadata for alpha arm\n",
    "    beta_meta : dict\n",
    "        Parsed metadata for beta arm\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing spatial information\n",
    "    \"\"\"\n",
    "    def parse_position(pos_str):\n",
    "        \"\"\"Parse position string like '-0 μm' or '0.1 μm'\"\"\"\n",
    "        try:\n",
    "            # Extract number from string\n",
    "            value = float(pos_str.split()[0])\n",
    "            return value\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    spatial_info = {\n",
    "        'pixel_size_um': {\n",
    "            'alpha': alpha_meta['pixel_size_um'],\n",
    "            'beta': beta_meta['pixel_size_um'],\n",
    "            'average': (alpha_meta['pixel_size_um'] + beta_meta['pixel_size_um']) / 2.0\n",
    "        },\n",
    "        'z_step_um': {\n",
    "            'alpha': alpha_meta['z_step_um'],\n",
    "            'beta': beta_meta['z_step_um'],\n",
    "            'average': (alpha_meta['z_step_um'] + beta_meta['z_step_um']) / 2.0\n",
    "        },\n",
    "        'position_x': {\n",
    "            'alpha': parse_position(alpha_meta['position_x']),\n",
    "            'beta': parse_position(beta_meta['position_x'])\n",
    "        },\n",
    "        'position_y': {\n",
    "            'alpha': parse_position(alpha_meta['position_y']),\n",
    "            'beta': parse_position(beta_meta['position_y'])\n",
    "        },\n",
    "        'mv_rotations': {\n",
    "            'alpha': alpha_meta['mv_rotations'],\n",
    "            'beta': beta_meta['mv_rotations']\n",
    "        },\n",
    "        'image_dimensions': {\n",
    "            'width': alpha_meta['width'],\n",
    "            'height': alpha_meta['height'],\n",
    "            'slices': min(alpha_meta['slices'], beta_meta['slices'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate physical dimensions\n",
    "    spatial_info['physical_dimensions_um'] = {\n",
    "        'xy': {\n",
    "            'width': spatial_info['image_dimensions']['width'] * spatial_info['pixel_size_um']['average'],\n",
    "            'height': spatial_info['image_dimensions']['height'] * spatial_info['pixel_size_um']['average']\n",
    "        },\n",
    "        'z': {\n",
    "            'depth': spatial_info['image_dimensions']['slices'] * spatial_info['z_step_um']['average']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return spatial_info\n",
    "\n",
    "# Test spatial info extraction\n",
    "if len(acquisitions) > 0:\n",
    "    acq = acquisitions[0]\n",
    "    alpha_meta = parse_metadata(acq['alpha_metadata'])\n",
    "    beta_meta = parse_metadata(acq['beta_metadata'])\n",
    "    \n",
    "    spatial = extract_spatial_info(alpha_meta, beta_meta)\n",
    "    print(\"Spatial information:\")\n",
    "    print(f\"  Pixel size: {spatial['pixel_size_um']['average']:.4f} μm/pixel\")\n",
    "    print(f\"  Z-step: {spatial['z_step_um']['average']:.4f} μm/slice\")\n",
    "    print(f\"  Physical volume: {spatial['physical_dimensions_um']['xy']['width']:.2f} × {spatial['physical_dimensions_um']['xy']['height']:.2f} × {spatial['physical_dimensions_um']['z']['depth']:.2f} μm\")\n",
    "    print(f\"  Alpha position: ({spatial['position_x']['alpha']:.3f}, {spatial['position_y']['alpha']:.3f}) μm\")\n",
    "    print(f\"  Beta position: ({spatial['position_x']['beta']:.3f}, {spatial['position_y']['beta']:.3f}) μm\")\n",
    "    print(f\"  MVRotations - Alpha: {spatial['mv_rotations']['alpha']}, Beta: {spatial['mv_rotations']['beta']}\")\n",
    "    if '90' in spatial['mv_rotations']['alpha']:\n",
    "        print(\"  Note: 90° rotation detected between arms (typical for diSPIM)\")\n",
    "else:\n",
    "    print(\"No acquisitions found for spatial info test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Video Creation Pipeline\n",
    "\n",
    "Create functions to generate side-by-side video sequences from alpha and beta data. The videos will:\n",
    "- Display alpha and beta channels side-by-side\n",
    "- Use proper frame rates based on slice periods\n",
    "- Normalize intensity for visualization\n",
    "- Support different channel selections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video creation functions defined!\n"
     ]
    }
   ],
   "source": [
    "def scale_image_for_display(img, vmin=None, vmax=None):\n",
    "    \"\"\"\n",
    "    Scale image for display without clipping data - preserves full dynamic range.\n",
    "    Uses linear scaling to 0-1 range for matplotlib display.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    img : numpy.ndarray\n",
    "        Input image (any dtype)\n",
    "    vmin : float or None\n",
    "        Minimum value for scaling (None = use image min)\n",
    "    vmax : float or None\n",
    "        Maximum value for scaling (None = use image max)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (scaled_image, vmin_used, vmax_used)\n",
    "        scaled_image is float64 in range [0, 1] for matplotlib\n",
    "    \"\"\"\n",
    "    if vmin is None:\n",
    "        vmin = float(img.min())\n",
    "    if vmax is None:\n",
    "        vmax = float(img.max())\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if vmax == vmin:\n",
    "        scaled = np.zeros_like(img, dtype=np.float64)\n",
    "    else:\n",
    "        scaled = (img.astype(np.float64) - vmin) / (vmax - vmin)\n",
    "        scaled = np.clip(scaled, 0, 1)\n",
    "    \n",
    "    return scaled, vmin, vmax\n",
    "\n",
    "def create_side_by_side_frame(alpha_slice, beta_slice, normalize=False, vmin_alpha=None, vmax_alpha=None, vmin_beta=None, vmax_beta=None):\n",
    "    \"\"\"\n",
    "    Create a side-by-side frame from alpha and beta slices.\n",
    "    Returns scaled images ready for matplotlib display (preserves full dynamic range).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha_slice : numpy.ndarray\n",
    "        Single slice from alpha arm (height, width)\n",
    "    beta_slice : numpy.ndarray\n",
    "        Single slice from beta arm (height, width)\n",
    "    normalize : bool\n",
    "        DEPRECATED - kept for compatibility but not used\n",
    "    vmin_alpha, vmax_alpha : float or None\n",
    "        Display range for alpha (None = use full range)\n",
    "    vmin_beta, vmax_beta : float or None\n",
    "        Display range for beta (None = use full range)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (side_by_side_image, vmin_alpha, vmax_alpha, vmin_beta, vmax_beta)\n",
    "        Image is float64 in [0,1] range for matplotlib imshow\n",
    "    \"\"\"\n",
    "    # Scale images for display (preserves full dynamic range)\n",
    "    alpha_scaled, vmin_a, vmax_a = scale_image_for_display(alpha_slice, vmin_alpha, vmax_alpha)\n",
    "    beta_scaled, vmin_b, vmax_b = scale_image_for_display(beta_slice, vmin_beta, vmax_beta)\n",
    "    \n",
    "    # Create side-by-side image\n",
    "    side_by_side = np.hstack([alpha_scaled, beta_scaled])\n",
    "    \n",
    "    return side_by_side, vmin_a, vmax_a, vmin_b, vmax_b\n",
    "\n",
    "def create_video_from_stacks(alpha_data, beta_data, output_path, \n",
    "                             channel_alpha=0, channel_beta=0,\n",
    "                             fps=10, max_slices=None, normalize=True):\n",
    "    \"\"\"\n",
    "    Create a video file from alpha and beta image stacks.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha_data : numpy.ndarray\n",
    "        Alpha image stack (slices, channels, height, width) or (slices, height, width)\n",
    "    beta_data : numpy.ndarray\n",
    "        Beta image stack (slices, channels, height, width) or (slices, height, width)\n",
    "    output_path : str or Path\n",
    "        Output video file path\n",
    "    channel_alpha : int\n",
    "        Channel index to use for alpha (if multi-channel)\n",
    "    channel_beta : int\n",
    "        Channel index to use for beta (if multi-channel)\n",
    "    fps : float\n",
    "        Frames per second for output video\n",
    "    max_slices : int or None\n",
    "        Maximum number of slices to include (for testing)\n",
    "    normalize : bool\n",
    "        Whether to normalize intensities\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Path to created video file\n",
    "    \"\"\"\n",
    "    # Extract channels if needed\n",
    "    if len(alpha_data.shape) == 4:\n",
    "        alpha_slices = alpha_data[:, channel_alpha, :, :]\n",
    "    else:\n",
    "        alpha_slices = alpha_data\n",
    "    \n",
    "    if len(beta_data.shape) == 4:\n",
    "        beta_slices = beta_data[:, channel_beta, :, :]\n",
    "    else:\n",
    "        beta_slices = beta_data\n",
    "    \n",
    "    # Limit slices if requested\n",
    "    num_slices = min(alpha_slices.shape[0], beta_slices.shape[0])\n",
    "    if max_slices is not None:\n",
    "        num_slices = min(num_slices, max_slices)\n",
    "    \n",
    "    alpha_slices = alpha_slices[:num_slices]\n",
    "    beta_slices = beta_slices[:num_slices]\n",
    "    \n",
    "    print(f\"Creating video with {num_slices} frames at {fps} fps...\")\n",
    "    \n",
    "    # Create frames\n",
    "    frames = []\n",
    "    for i in range(num_slices):\n",
    "        frame, _, _, _, _ = create_side_by_side_frame(alpha_slices[i], beta_slices[i], normalize=normalize)\n",
    "        # Convert to uint8 for video (0-255 range)\n",
    "        frame_uint8 = (frame * 255).astype(np.uint8)\n",
    "        # Convert to RGB for video\n",
    "        if len(frame_uint8.shape) == 2:\n",
    "            frame_uint8 = np.stack([frame_uint8, frame_uint8, frame_uint8], axis=-1)\n",
    "        frames.append(frame_uint8)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Processed {i + 1}/{num_slices} frames...\")\n",
    "    \n",
    "    # Save video\n",
    "    print(f\"Saving video to {output_path}...\")\n",
    "    imageio.mimwrite(str(output_path), frames, fps=fps, codec='libx264', quality=8)\n",
    "    \n",
    "    print(f\"Video saved successfully!\")\n",
    "    return str(output_path)\n",
    "\n",
    "print(\"Video creation functions defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Interactive Visualization\n",
    "\n",
    "Create interactive widgets for exploring the data:\n",
    "- Z-slice selector slider\n",
    "- Channel selector\n",
    "- Playback controls\n",
    "- Side-by-side display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive viewer class defined!\n"
     ]
    }
   ],
   "source": [
    "class InteractiveViewer:\n",
    "    \"\"\"\n",
    "    Interactive viewer for diSPIM data with widgets.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha_data, beta_data, alpha_meta=None, beta_meta=None):\n",
    "        \"\"\"\n",
    "        Initialize the viewer with data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha_data : numpy.ndarray\n",
    "            Alpha image stack\n",
    "        beta_data : numpy.ndarray\n",
    "            Beta image stack\n",
    "        alpha_meta : dict, optional\n",
    "            Alpha metadata\n",
    "        beta_meta : dict, optional\n",
    "            Beta metadata\n",
    "        \"\"\"\n",
    "        self.alpha_data = alpha_data\n",
    "        self.beta_data = beta_data\n",
    "        self.alpha_meta = alpha_meta or {}\n",
    "        self.beta_meta = beta_meta or {}\n",
    "        \n",
    "        # Determine data structure\n",
    "        if len(alpha_data.shape) == 4:\n",
    "            self.num_slices, self.num_channels_alpha, self.height, self.width = alpha_data.shape\n",
    "            self.alpha_channels = self.alpha_meta.get('channel_names', [f'Channel {i}' for i in range(self.num_channels_alpha)])\n",
    "        else:\n",
    "            self.num_slices, self.height, self.width = alpha_data.shape\n",
    "            self.num_channels_alpha = 1\n",
    "            self.alpha_channels = ['Channel 0']\n",
    "        \n",
    "        if len(beta_data.shape) == 4:\n",
    "            _, self.num_channels_beta, _, _ = beta_data.shape\n",
    "            self.beta_channels = self.beta_meta.get('channel_names', [f'Channel {i}' for i in range(self.num_channels_beta)])\n",
    "        else:\n",
    "            self.num_channels_beta = 1\n",
    "            self.beta_channels = ['Channel 0']\n",
    "        \n",
    "        # Create figure and axes\n",
    "        self.fig, self.ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "        self.ax.axis('off')\n",
    "        \n",
    "        # Initialize image object (will be created on first update)\n",
    "        self.im = None\n",
    "        self.text_alpha = None\n",
    "        self.text_beta = None\n",
    "        self.text_slice = None\n",
    "        \n",
    "    def update_display(self, slice_idx, channel_alpha=0, channel_beta=0):\n",
    "        \"\"\"\n",
    "        Update the display with a specific slice and channels.\n",
    "        \"\"\"\n",
    "        # Extract slices\n",
    "        if len(self.alpha_data.shape) == 4:\n",
    "            alpha_slice = self.alpha_data[slice_idx, channel_alpha, :, :]\n",
    "        else:\n",
    "            alpha_slice = self.alpha_data[slice_idx, :, :]\n",
    "        \n",
    "        if len(self.beta_data.shape) == 4:\n",
    "            beta_slice = self.beta_data[slice_idx, channel_beta, :, :]\n",
    "        else:\n",
    "            beta_slice = self.beta_data[slice_idx, :, :]\n",
    "        \n",
    "        # Create side-by-side frame (preserves full dynamic range)\n",
    "        frame, vmin_a, vmax_a, vmin_b, vmax_b = create_side_by_side_frame(\n",
    "            alpha_slice, beta_slice, normalize=False\n",
    "        )\n",
    "        \n",
    "        # Update or create image\n",
    "        if self.im is None:\n",
    "            # First time: create image\n",
    "            self.im = self.ax.imshow(frame, cmap='gray', vmin=0, vmax=1, aspect='equal')\n",
    "            self.ax.axis('off')\n",
    "        else:\n",
    "            # Update existing image data\n",
    "            self.im.set_data(frame)\n",
    "        \n",
    "        # Update or create text labels\n",
    "        alpha_range = f\"[{vmin_a:.0f}, {vmax_a:.0f}]\"\n",
    "        beta_range = f\"[{vmin_b:.0f}, {vmax_b:.0f}]\"\n",
    "        \n",
    "        if self.text_alpha is None:\n",
    "            # Create text labels\n",
    "            self.text_alpha = self.ax.text(\n",
    "                self.width // 2, 30,\n",
    "                f'Alpha {alpha_range}',\n",
    "                ha='center', va='top', color='white', fontsize=12, weight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='black', alpha=0.7)\n",
    "            )\n",
    "            self.text_beta = self.ax.text(\n",
    "                self.width + self.width // 2, 30,\n",
    "                f'Beta {beta_range}',\n",
    "                ha='center', va='top', color='white', fontsize=12, weight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='black', alpha=0.7)\n",
    "            )\n",
    "            self.text_slice = self.ax.text(\n",
    "                self.width, self.height - 30,\n",
    "                f'Slice {slice_idx}/{self.num_slices-1}',\n",
    "                ha='center', va='bottom', color='white', fontsize=12,\n",
    "                bbox=dict(boxstyle='round', facecolor='black', alpha=0.7)\n",
    "            )\n",
    "        else:\n",
    "            # Update existing text\n",
    "            self.text_alpha.set_text(f'Alpha {alpha_range}')\n",
    "            self.text_beta.set_text(f'Beta {beta_range}')\n",
    "            self.text_slice.set_text(f'Slice {slice_idx}/{self.num_slices-1}')\n",
    "        \n",
    "        # Redraw only what's necessary\n",
    "        self.fig.canvas.draw_idle()  # Use draw_idle for better widget responsiveness\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        \"\"\"\n",
    "        Create interactive widgets.\n",
    "        \"\"\"\n",
    "        slice_slider = widgets.IntSlider(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=self.num_slices - 1,\n",
    "            step=1,\n",
    "            description='Slice:',\n",
    "            continuous_update=True,\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='500px')\n",
    "        )\n",
    "        \n",
    "        channel_alpha_dropdown = widgets.Dropdown(\n",
    "            options=list(range(self.num_channels_alpha)),\n",
    "            value=0,\n",
    "            description='Alpha Channel:',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        channel_beta_dropdown = widgets.Dropdown(\n",
    "            options=list(range(self.num_channels_beta)),\n",
    "            value=0,\n",
    "            description='Beta Channel:',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        # Create output widget to capture display updates\n",
    "        output = widgets.Output(layout={'border': '1px solid black', 'height': '600px'})\n",
    "        \n",
    "        # Display figure once in output\n",
    "        with output:\n",
    "            self.update_display(0, 0, 0)\n",
    "            display(self.fig)\n",
    "        \n",
    "        def on_change(slice_idx, ch_alpha, ch_beta):\n",
    "            # Update the display (figure is already shown, just update data)\n",
    "            self.update_display(slice_idx, ch_alpha, ch_beta)\n",
    "        \n",
    "        # Create interactive widget\n",
    "        interactive_widget = interactive(\n",
    "            on_change,\n",
    "            slice_idx=slice_slider,\n",
    "            ch_alpha=channel_alpha_dropdown,\n",
    "            ch_beta=channel_beta_dropdown\n",
    "        )\n",
    "        \n",
    "        # Combine widgets vertically\n",
    "        return widgets.VBox([\n",
    "            widgets.HBox([slice_slider]),\n",
    "            widgets.HBox([channel_alpha_dropdown, channel_beta_dropdown]),\n",
    "            output\n",
    "        ])\n",
    "\n",
    "print(\"Interactive viewer class defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Example Usage\n",
    "\n",
    "Now let's put it all together! We'll:\n",
    "1. Select an acquisition pair\n",
    "2. Load the data (with optional downsampling for initial exploration)\n",
    "3. Display some frames\n",
    "4. Create a sample video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected acquisition: 10msec_worm/I\n",
      "  Alpha: beads_alpha_worm2_4\n",
      "  Beta: beads_beta_worm2_4\n"
     ]
    }
   ],
   "source": [
    "# Select an acquisition to work with\n",
    "# You can change this index to select a different acquisition\n",
    "ACQUISITION_INDEX = 0  # 0 = first acquisition, 1 = second, etc.\n",
    "\n",
    "if len(acquisitions) > ACQUISITION_INDEX:\n",
    "    selected_acq = acquisitions[ACQUISITION_INDEX]\n",
    "    print(f\"Selected acquisition: {selected_acq['condition']}/{selected_acq['run']}\")\n",
    "    print(f\"  Alpha: {selected_acq['alpha_path'].name}\")\n",
    "    print(f\"  Beta: {selected_acq['beta_path'].name}\")\n",
    "else:\n",
    "    print(f\"Error: Acquisition index {ACQUISITION_INDEX} not available. Found {len(acquisitions)} acquisitions.\")\n",
    "    selected_acq = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "\n",
      "Alpha metadata summary:\n",
      "  Dimensions: 2304×2304×200\n",
      "  Channels: 2 (HamCam2, HamCam1)\n",
      "  Start time: 2025-11-12 16:49:53 -0500\n",
      "\n",
      "Beta metadata summary:\n",
      "  Dimensions: 2304×2304×200\n",
      "  Channels: 2 (HamuHam4, HamuHam3)\n",
      "  Start time: 2025-11-12 16:49:51 -0500\n",
      "\n",
      "Temporal alignment:\n",
      "  Time offset: -2.000 seconds\n",
      "  Average slice period: 21.50 ms\n",
      "\n",
      "Spatial information:\n",
      "  Pixel size: 0.1220 μm/pixel\n",
      "  Z-step: 0.7000 μm/slice\n",
      "  Volume size: 281.09 × 281.09 × 140.00 μm\n"
     ]
    }
   ],
   "source": [
    "# Load metadata for selected acquisition\n",
    "if selected_acq:\n",
    "    print(\"Loading metadata...\")\n",
    "    alpha_meta = parse_metadata(selected_acq['alpha_metadata'])\n",
    "    beta_meta = parse_metadata(selected_acq['beta_metadata'])\n",
    "    \n",
    "    print(\"\\nAlpha metadata summary:\")\n",
    "    print(f\"  Dimensions: {alpha_meta['width']}×{alpha_meta['height']}×{alpha_meta['slices']}\")\n",
    "    print(f\"  Channels: {alpha_meta['channels']} ({', '.join(alpha_meta['channel_names'])})\")\n",
    "    print(f\"  Start time: {alpha_meta['start_time']}\")\n",
    "    \n",
    "    print(\"\\nBeta metadata summary:\")\n",
    "    print(f\"  Dimensions: {beta_meta['width']}×{beta_meta['height']}×{beta_meta['slices']}\")\n",
    "    print(f\"  Channels: {beta_meta['channels']} ({', '.join(beta_meta['channel_names'])})\")\n",
    "    print(f\"  Start time: {beta_meta['start_time']}\")\n",
    "    \n",
    "    # Calculate temporal alignment\n",
    "    alignment = calculate_temporal_alignment(alpha_meta, beta_meta)\n",
    "    print(f\"\\nTemporal alignment:\")\n",
    "    print(f\"  Time offset: {alignment['time_offset_sec']:.3f} seconds\")\n",
    "    print(f\"  Average slice period: {alignment['slice_period_ms']:.2f} ms\")\n",
    "    \n",
    "    # Extract spatial info\n",
    "    spatial = extract_spatial_info(alpha_meta, beta_meta)\n",
    "    print(f\"\\nSpatial information:\")\n",
    "    print(f\"  Pixel size: {spatial['pixel_size_um']['average']:.4f} μm/pixel\")\n",
    "    print(f\"  Z-step: {spatial['z_step_um']['average']:.4f} μm/slice\")\n",
    "    print(f\"  Volume size: {spatial['physical_dimensions_um']['xy']['width']:.2f} × {spatial['physical_dimensions_um']['xy']['height']:.2f} × {spatial['physical_dimensions_um']['z']['depth']:.2f} μm\")\n",
    "else:\n",
    "    print(\"No acquisition selected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Data\n",
    "\n",
    "**Note**: Loading full stacks can be memory-intensive. For initial exploration, we'll load a subset of slices. \n",
    "You can adjust `MAX_SLICES_FOR_DISPLAY` to load more or fewer slices. Set to `None` to load all slices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OME-TIFF files...\n",
      "This may take a few moments for large files...\n",
      "\n",
      "Loading alpha data...\n",
      "Loading OME-TIFF: 10msec_worm/I/beads_alpha_worm2_4/beads_alpha_worm2_MMStack_Pos0.ome.tif\n",
      "Raw data shape: (200, 2, 2304, 2304)\n",
      "Data dtype: uint16\n",
      "Final data shape: (200, 2, 2304, 2304)\n",
      "\n",
      "Loading beta data...\n",
      "Loading OME-TIFF: 10msec_worm/I/beads_beta_worm2_4/beads_beta_worm2_MMStack_Pos0.ome.tif\n",
      "Raw data shape: (200, 2, 2304, 2304)\n",
      "Data dtype: uint16\n",
      "Final data shape: (200, 2, 2304, 2304)\n",
      "\n",
      "Data loaded successfully!\n",
      "Alpha data shape: (200, 2, 2304, 2304)\n",
      "Beta data shape: (200, 2, 2304, 2304)\n",
      "Alpha data dtype: uint16, range: [17, 12712]\n",
      "Beta data dtype: uint16, range: [0, 11448]\n"
     ]
    }
   ],
   "source": [
    "# Set maximum slices to load (None = load all, smaller number = faster loading for testing)\n",
    "MAX_SLICES_FOR_DISPLAY = None  # Load full stack  # Change this to load more/fewer slices\n",
    "\n",
    "if selected_acq:\n",
    "    print(\"Loading OME-TIFF files...\")\n",
    "    print(\"This may take a few moments for large files...\")\n",
    "    \n",
    "    # Load alpha data (first channel, limited slices)\n",
    "    print(\"\\nLoading alpha data...\")\n",
    "    alpha_data = load_ome_tiff(\n",
    "        selected_acq['alpha_tiff'], \n",
    "        metadata=alpha_meta,\n",
    "        channel_idx=None,  # Load all channels\n",
    "        max_slices=MAX_SLICES_FOR_DISPLAY\n",
    "    )\n",
    "    \n",
    "    # Load beta data (first channel, limited slices)\n",
    "    print(\"\\nLoading beta data...\")\n",
    "    beta_data = load_ome_tiff(\n",
    "        selected_acq['beta_tiff'],\n",
    "        metadata=beta_meta,\n",
    "        channel_idx=None,  # Load all channels\n",
    "        max_slices=MAX_SLICES_FOR_DISPLAY\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData loaded successfully!\")\n",
    "    print(f\"Alpha data shape: {alpha_data.shape}\")\n",
    "    print(f\"Beta data shape: {beta_data.shape}\")\n",
    "    print(f\"Alpha data dtype: {alpha_data.dtype}, range: [{alpha_data.min()}, {alpha_data.max()}]\")\n",
    "    print(f\"Beta data dtype: {beta_data.dtype}, range: [{beta_data.min()}, {beta_data.max()}]\")\n",
    "else:\n",
    "    print(\"No acquisition selected.\")\n",
    "    alpha_data = None\n",
    "    beta_data = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Sample Frames\n",
    "\n",
    "Let's visualize a few slices side-by-side to get a sense of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        // There's no need to resize if the WebSocket is not connected:\n        // - If it is still connecting, then we will get an initial resize from\n        //   Python once it connects.\n        // - If it has disconnected, then resizing will clear the canvas and\n        //   never get anything back to refill it, so better to not resize and\n        //   keep something visible.\n        if (fig.ws.readyState != 1) {\n            return;\n        }\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '%s' message type: \",\n                msg_type,\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '%s' message: \", msg_type, msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_%s' callback:\",\n                    msg_type,\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        buttons: event.buttons,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='31662131-6689-4e44-a0c6-0daad0dbd29e'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if alpha_data is not None and beta_data is not None:\n",
    "    # Display a few sample slices\n",
    "    num_samples = min(5, alpha_data.shape[0])\n",
    "    sample_indices = np.linspace(0, alpha_data.shape[0] - 1, num_samples, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, slice_idx in enumerate(sample_indices):\n",
    "        # Extract slices (first channel)\n",
    "        if len(alpha_data.shape) == 4:\n",
    "            alpha_slice = alpha_data[slice_idx, 0, :, :]\n",
    "            beta_slice = beta_data[slice_idx, 0, :, :]\n",
    "        else:\n",
    "            alpha_slice = alpha_data[slice_idx, :, :]\n",
    "            beta_slice = beta_data[slice_idx, :, :]\n",
    "        \n",
    "        # Create side-by-side frame\n",
    "        frame, _, _, _, _ = create_side_by_side_frame(alpha_slice, beta_slice, normalize=False)\n",
    "        \n",
    "        axes[i].imshow(frame, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Slice {slice_idx}', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f'Sample slices from {selected_acq[\"condition\"]}/{selected_acq[\"run\"]}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data loaded to display.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Exploration\n",
    "\n",
    "Use the widgets below to explore different slices and channels interactively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        // There's no need to resize if the WebSocket is not connected:\n        // - If it is still connecting, then we will get an initial resize from\n        //   Python once it connects.\n        // - If it has disconnected, then resizing will clear the canvas and\n        //   never get anything back to refill it, so better to not resize and\n        //   keep something visible.\n        if (fig.ws.readyState != 1) {\n            return;\n        }\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '%s' message type: \",\n                msg_type,\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '%s' message: \", msg_type, msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_%s' callback:\",\n                    msg_type,\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        buttons: event.buttons,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='32158e0e-e422-42c7-b8f3-8c94ce86679a'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b6b00fa7564fce8689eed6f0d6859f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='Slice:', layout=Layout(width='500px'), max=199, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if alpha_data is not None and beta_data is not None:\n",
    "    # Create interactive viewer\n",
    "    viewer = InteractiveViewer(alpha_data, beta_data, alpha_meta, beta_meta)\n",
    "    \n",
    "    # Create and display widgets (figure will be shown in output widget)\n",
    "    interactive_widget = viewer.create_widgets()\n",
    "    display(interactive_widget)\n",
    "else:\n",
    "    print(\"No data loaded for interactive viewing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Video Output\n",
    "\n",
    "Create a video file showing the alpha and beta channels side-by-side. The video will be saved in the current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video at 30.00 fps (based on 21.50 ms slice period)...\n",
      "Creating video with 200 frames at 30.0 fps...\n",
      "  Processed 50/200 frames...\n",
      "  Processed 100/200 frames...\n",
      "  Processed 150/200 frames...\n",
      "  Processed 200/200 frames...\n",
      "Saving video to 10msec_worm_I_alpha_beta_video.mp4...\n",
      "Video saved successfully!\n",
      "\n",
      "Video saved to: 10msec_worm_I_alpha_beta_video.mp4\n",
      "You can play this video with any standard video player.\n"
     ]
    }
   ],
   "source": [
    "if alpha_data is not None and beta_data is not None:\n",
    "    # Calculate appropriate frame rate based on slice period\n",
    "    slice_period_ms = alignment['slice_period_ms']\n",
    "    fps = 1000.0 / slice_period_ms  # frames per second\n",
    "    # Cap at reasonable frame rate for video playback\n",
    "    fps = min(fps, 30.0)\n",
    "    \n",
    "    print(f\"Creating video at {fps:.2f} fps (based on {slice_period_ms:.2f} ms slice period)...\")\n",
    "    \n",
    "    # Create output filename\n",
    "    output_filename = f\"{selected_acq['condition']}_{selected_acq['run']}_alpha_beta_video.mp4\"\n",
    "    \n",
    "    # Create video (using first channel for both arms)\n",
    "    video_path = create_video_from_stacks(\n",
    "        alpha_data, \n",
    "        beta_data,\n",
    "        output_filename,\n",
    "        channel_alpha=0,\n",
    "        channel_beta=0,\n",
    "        fps=fps,\n",
    "        max_slices=None,  # Use all loaded slices\n",
    "        normalize=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nVideo saved to: {video_path}\")\n",
    "    print(f\"You can play this video with any standard video player.\")\n",
    "else:\n",
    "    print(\"No data loaded to create video.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **Metadata Parsing**: Successfully extracted acquisition parameters, dimensions, timing, and spatial information\n",
    "2. **Data Loading**: Loaded OME-TIFF files with proper handling of multi-dimensional arrays\n",
    "3. **Temporal Alignment**: Calculated timing offsets and frame alignment between alpha and beta arms\n",
    "4. **Spatial Information**: Extracted pixel size, z-step, and position information for future registration\n",
    "5. **Visualization**: Created side-by-side displays and interactive exploration tools\n",
    "6. **Video Creation**: Generated video files for easy viewing\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- **Data Structure**: Each arm has 2 channels (two cameras), 200 slices, 2304×2304 pixels\n",
    "- **Temporal Info**: Slice periods vary by acquisition condition (1ms, 10ms, 20ms, 200ms)\n",
    "- **Spatial Calibration**: ~0.244 μm/pixel in XY, 0.7 μm between slices\n",
    "- **Rotation**: 90° rotation between arms (MVRotations: \"0_90\") - important for future registration\n",
    "\n",
    "### Future Work\n",
    "\n",
    "1. **Image Registration**: Use spatial information to align/register alpha and beta volumes\n",
    "2. **Fusion**: Combine registered alpha and beta data for improved resolution\n",
    "3. **Segmentation**: Apply ML-based segmentation tools\n",
    "4. **Multi-timepoint**: Extend to handle time-lapse data (currently single timepoint)\n",
    "5. **Full Stack Loading**: Load complete stacks for full-resolution analysis\n",
    "\n",
    "### Notes\n",
    "\n",
    "- For memory efficiency, consider loading subsets of slices or downsampling for initial exploration\n",
    "- The metadata contains extensive frame-by-frame information that could be used for more detailed temporal analysis\n",
    "- Spatial positions (Position_X, Position_Y) can be used to align acquisitions from different positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
